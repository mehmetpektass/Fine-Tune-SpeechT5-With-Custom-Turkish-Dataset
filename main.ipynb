{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(\"main_dataset_folder.zip\", \"r\") as zip_ref:\n",
    "  zip_ref.extractall(\"./\")\n",
    "print(\"Extraction of zip file complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import  load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"/content/main_dataset_folder/main_dataset.csv\",\n",
    "    split = \"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"Unnamed: 0\", \"id\", \"filename\", \"start\", \"end\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AUDIO_ROOT = \"/main_dataset_folder\"\n",
    "\n",
    "def fix_path(example):\n",
    "  example[\"file_path\"] = os.path.join(AUDIO_ROOT, example[\"file_path\"])\n",
    "  return example\n",
    "\n",
    "dataset = dataset.map(fix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "def load_audio_with_librosa(example):\n",
    "    try:\n",
    "        audio_array, sample_rate = librosa.load(\n",
    "            example[\"file_path\"],\n",
    "            sr=16000,  # 16kHz resample\n",
    "            mono=True,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        audio_array = audio_array.astype(np.float32)\n",
    "\n",
    "        example[\"audio\"] = {\n",
    "            \"array\": audio_array,\n",
    "            \"sampling_rate\": 16000\n",
    "        }\n",
    "\n",
    "        return example\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {example['file_path']}: {e}\")\n",
    "    \n",
    "        example[\"audio\"] = {\n",
    "            \"array\": np.zeros(16000, dtype=np.float32),\n",
    "            \"sampling_rate\": 16000\n",
    "        }\n",
    "        return example\n",
    "\n",
    "dataset = dataset.map(load_audio_with_librosa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"file_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  texts = [str(text) for text in batch[\"text\"] if text is not None]\n",
    "  all_text = \" \".join(texts)\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched = True,\n",
    "    batch_size = -1,\n",
    "    keep_in_memory = True,\n",
    "    remove_columns = dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab- tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"][5696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "  if text is None or not str(text).strip():\n",
    "    return \"\"\n",
    "\n",
    "  text = str(text).lower()\n",
    "\n",
    "  text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "  text = ' '.join(text.split())\n",
    "\n",
    "  return text\n",
    "\n",
    "def add_normalized_text(example):\n",
    "  example[\"normalized_text\"] = normalize_text(example[\"text\"])\n",
    "\n",
    "  return example\n",
    "\n",
    "dataset = dataset.map(add_normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab - tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"â\", \"a\"),\n",
    "    (\"ç\", \"ch\"),\n",
    "    (\"ğ\", \"gh\"),\n",
    "    (\"ı\", \"i\"),\n",
    "    (\"î\", \"i\"),\n",
    "    (\"ö\", \"oe\"),\n",
    "    (\"ş\", \"sh\"),\n",
    "    (\"ü\", \"ue\"),\n",
    "    (\"û\", \"u\"),\n",
    "]\n",
    "\n",
    "def cleanup_text(inputs):\n",
    "  for src, dst in replacements:\n",
    "    inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
    "  return inputs\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio_data = example[\"audio\"]\n",
    "\n",
    "    processed_example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio_data[\"array\"],\n",
    "        sampling_rate=audio_data[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    processed_example[\"labels\"] = processed_example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    processed_example[\"speaker_embeddings\"] = create_speaker_embedding(audio_data[\"array\"])\n",
    "\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"speecht5_finetuned_tts_tr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=150,\n",
    "    max_steps=800,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
    "    \"{your name here}/{your model name here}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][304]\n",
    "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Merhaba, ben Mehmet. 21 yaşındayım. Umarım bu ses kaydı anlaşılırdır ve bu proje de bitmiştir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words = {\n",
    "    0: \"sıfır\", 1: \"bir\", 2: \"iki\", 3: \"üç\", 4: \"dört\", 5: \"beş\", 6: \"altı\", 7: \"yedi\", 8: \"sekiz\", 9: \"dokuz\",\n",
    "    10: \"on\", 11: \"on bir\", 12: \"on iki\", 13: \"on üç\", 14: \"on dört\", 15: \"on beş\", 16: \"on altı\", 17: \"on yedi\",\n",
    "    18: \"on sekiz\", 19: \"on dokuz\", 20: \"yirmi\", 30: \"otuz\", 40: \"kırk\", 50: \"elli\", 60: \"altmış\", 70: \"yetmiş\",\n",
    "    80: \"seksen\", 90: \"doksan\", 100: \"yüz\", 1000: \"bin\"\n",
    "}\n",
    "\n",
    "def number_to_words(number):\n",
    "    if number < 20:\n",
    "        return number_words[number]\n",
    "    elif number < 100:\n",
    "        tens, unit = divmod(number, 10)\n",
    "        return number_words[tens * 10] + (\" \" + number_words[unit] if unit else \"\")\n",
    "    elif number < 1000:\n",
    "        hundreds, remainder = divmod(number, 100)\n",
    "        return (number_words[hundreds] + \" yüz\" if hundreds > 1 else \"yüz\") + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
    "    elif number < 1000000:\n",
    "        thousands, remainder = divmod(number, 1000)\n",
    "        return (number_to_words(thousands) + \" bin\" if thousands > 1 else \"bin\") + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
    "    elif number < 1000000000:\n",
    "        millions, remainder = divmod(number, 1000000)\n",
    "        return number_to_words(millions) + \" milyon\" + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
    "    elif number < 1000000000000:\n",
    "        billions, remainder = divmod(number, 1000000000)\n",
    "        return number_to_words(billions) + \" milyar\" + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
    "    else:\n",
    "        return str(number)\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "\n",
    "    def replace(match):\n",
    "        number = int(match.group())\n",
    "        return number_to_words(number)\n",
    "\n",
    "    # Find the numbers and change with words.\n",
    "    result = re.sub(r'\\b\\d+\\b', replace, text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up text using the replacement pairs\n",
    "def cleanup_text(text):\n",
    "    for src, dst in replacements:\n",
    "        text = text.replace(src, dst)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "converted_text = replace_numbers_with_words(text)\n",
    "cleaned_text = cleanup_text(converted_text)\n",
    "final_text = normalize_text(cleaned_text)\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=final_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "\n",
    "Audio(speech.numpy(), rate=16000)\n",
    "# Save the audio to a file (e.g., 'output.wav')\n",
    "sf.write('output.wav', speech.numpy(), 16000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
