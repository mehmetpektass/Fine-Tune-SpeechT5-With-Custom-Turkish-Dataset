{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(\"main_dataset_folder.zip\", \"r\") as zip_ref:\n",
    "  zip_ref.extractall(\"./\")\n",
    "print(\"Extraction of zip file complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import  load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"/content/main_dataset_folder/main_dataset.csv\",\n",
    "    split = \"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"Unnamed: 0\", \"id\", \"filename\", \"start\", \"end\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AUDIO_ROOT = \"/main_dataset_folder\"\n",
    "\n",
    "def fix_path(example):\n",
    "  example[\"file_path\"] = os.path.join(AUDIO_ROOT, example[\"file_path\"])\n",
    "  return example\n",
    "\n",
    "dataset = dataset.map(fix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "def load_audio_with_librosa(example):\n",
    "    try:\n",
    "        audio_array, sample_rate = librosa.load(\n",
    "            example[\"file_path\"],\n",
    "            sr=16000,  # 16kHz resample\n",
    "            mono=True,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        audio_array = audio_array.astype(np.float32)\n",
    "\n",
    "        example[\"audio\"] = {\n",
    "            \"array\": audio_array,\n",
    "            \"sampling_rate\": 16000\n",
    "        }\n",
    "\n",
    "        return example\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {example['file_path']}: {e}\")\n",
    "    \n",
    "        example[\"audio\"] = {\n",
    "            \"array\": np.zeros(16000, dtype=np.float32),\n",
    "            \"sampling_rate\": 16000\n",
    "        }\n",
    "        return example\n",
    "\n",
    "dataset = dataset.map(load_audio_with_librosa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"file_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  texts = [str(text) for text in batch[\"text\"] if text is not None]\n",
    "  all_text = \" \".join(texts)\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched = True,\n",
    "    batch_size = -1,\n",
    "    keep_in_memory = True,\n",
    "    remove_columns = dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab- tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"][5696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "  if text is None or not str(text).strip():\n",
    "    return \"\"\n",
    "\n",
    "  text = str(text).lower()\n",
    "\n",
    "  text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "  text = ' '.join(text.split())\n",
    "\n",
    "  return text\n",
    "\n",
    "def add_normalized_text(example):\n",
    "  example[\"normalized_text\"] = normalize_text(example[\"text\"])\n",
    "\n",
    "  return example\n",
    "\n",
    "dataset = dataset.map(add_normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab - tokenizer_vocab"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
